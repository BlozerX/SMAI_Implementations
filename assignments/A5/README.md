# Assignment 5 — Density Estimation, Dynamical Systems, Time-Series Forecasting, Generative Models

This folder contains my notebooks for Assignment 5.

---

## Overview

Assignment 5 focuses on advanced topics spanning statistical modeling and deep learning. The emphasis is on learning probability distributions, identifying data-driven dynamical systems, forecasting real-world time series, and building generative models, with careful attention to evaluation methodology and interpretation of results.

---

## What I did

### 1) Kernel Density Estimation (KDE) for foreground detection
I implemented Kernel Density Estimation from scratch and used it as a non-parametric density model for background modeling in images. Foreground detection was framed as an outlier detection problem under the learned background distribution.

This included:
- analyzing the effect of kernel choice and bandwidth on smoothness and overfitting
- selecting density thresholds for separating foreground and background
- qualitatively evaluating segmentation results across different settings

---

### 2) Learning discrete-time dynamical systems from data
I modeled noisy time-series generated by unknown autonomous systems and learned data-driven recurrences using fixed-length history windows.

My work focused on:
- converting sequences into supervised learning problems (history → next value)
- selecting and justifying the recurrence order
- comparing baseline predictors with neural sequence models
- evaluating both one-step prediction error and long-horizon autoregressive rollouts to study stability

---

### 3) Time-series forecasting on real-world data
I performed time-series forecasting on cumulative GitHub star counts for repositories.

This involved:
- careful temporal train/validation/test splits to avoid data leakage
- appropriate preprocessing and scaling for cumulative time series
- comparing classical statistical models with neural forecasting models
- evaluating performance across multiple forecast horizons and analyzing error growth

---

### 4) Variational Autoencoders (VAE) for image generation
I implemented a Variational Autoencoder for image data and studied the trade-off between reconstruction quality and latent regularization.

I explored:
- stochastic latent representations using the reparameterization trick
- balancing reconstruction loss with KL-divergence
- effects of KL weighting on latent structure, diversity, and sample quality
- qualitative and quantitative evaluation of generated samples

---

## Notebook map

- `1.ipynb` — KDE implementation and foreground detection experiments  
- `2.ipynb` — learning discrete-time recurrences from sequence data  
- `3.ipynb` — time-series forecasting on GitHub star data  
- `4.ipynb` — Variational Autoencoder implementation and analysis  

---

## How to run

```bash
pip install numpy pandas matplotlib scikit-learn jupyter
pip install torch torchvision tqdm
pip install opencv-python
