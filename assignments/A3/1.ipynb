{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e276a2ca",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c241b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # Use non-interactive backend to prevent GUI issues.\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c16e40",
   "metadata": {},
   "source": [
    "## Image Processing and Dataset Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52e9d963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Section 1.1: Dataset Creation\n",
      "Dataset created from 'Dataset/border.png' with 2500 points.\n",
      "\n",
      "Executing Section 1.2: Main Model Training\n",
      "Model created. Starting training on border map...\n",
      "Saving results to runs/20251013-175009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 119.61it/s, loss=0.4729]\n",
      "Epoch 2/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 248.47it/s, loss=0.4900]\n",
      "Epoch 3/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 128.60it/s, loss=0.4701]\n",
      "Epoch 4/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 224.35it/s, loss=0.4680]\n",
      "Epoch 5/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 194.72it/s, loss=0.4201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/250 | Avg Loss: 0.4446 | Accuracy: 0.8004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 103.74it/s, loss=0.3226]\n",
      "Epoch 7/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 198.21it/s, loss=0.4635]\n",
      "Epoch 8/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 171.52it/s, loss=0.4122]\n",
      "Epoch 9/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 212.93it/s, loss=0.3992]\n",
      "Epoch 10/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 215.67it/s, loss=0.3172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/250 | Avg Loss: 0.3987 | Accuracy: 0.8164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 164.68it/s, loss=0.4421]\n",
      "Epoch 12/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 241.48it/s, loss=0.3959]\n",
      "Epoch 13/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 182.83it/s, loss=0.4175]\n",
      "Epoch 14/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 212.02it/s, loss=0.4112]\n",
      "Epoch 15/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 210.04it/s, loss=0.3997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/250 | Avg Loss: 0.3712 | Accuracy: 0.8288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 188.05it/s, loss=0.3238]\n",
      "Epoch 17/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 120.87it/s, loss=0.3705]\n",
      "Epoch 18/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 100.09it/s, loss=0.4412]\n",
      "Epoch 19/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 64.95it/s, loss=0.3283]\n",
      "Epoch 20/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 38.99it/s, loss=0.2899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/250 | Avg Loss: 0.3533 | Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 231.91it/s, loss=0.3651]\n",
      "Epoch 22/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 168.44it/s, loss=0.3134]\n",
      "Epoch 23/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 218.67it/s, loss=0.3493]\n",
      "Epoch 24/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 256.36it/s, loss=0.3758]\n",
      "Epoch 25/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 236.48it/s, loss=0.3432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/250 | Avg Loss: 0.3402 | Accuracy: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 218.99it/s, loss=0.2717]\n",
      "Epoch 27/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 221.56it/s, loss=0.3948]\n",
      "Epoch 28/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 334.89it/s, loss=0.2400]\n",
      "Epoch 29/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 206.74it/s, loss=0.3337]\n",
      "Epoch 30/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 228.57it/s, loss=0.3313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/250 | Avg Loss: 0.3295 | Accuracy: 0.8556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 204.68it/s, loss=0.2635]\n",
      "Epoch 32/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 228.98it/s, loss=0.3332]\n",
      "Epoch 33/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 288.96it/s, loss=0.3000]\n",
      "Epoch 34/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 210.38it/s, loss=0.3447]\n",
      "Epoch 35/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 256.21it/s, loss=0.3867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/250 | Avg Loss: 0.3204 | Accuracy: 0.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 170.29it/s, loss=0.4026]\n",
      "Epoch 37/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 239.88it/s, loss=0.2874]\n",
      "Epoch 38/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 247.46it/s, loss=0.4362]\n",
      "Epoch 39/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 201.70it/s, loss=0.4223]\n",
      "Epoch 40/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 237.08it/s, loss=0.3239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/250 | Avg Loss: 0.3128 | Accuracy: 0.8696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 274.53it/s, loss=0.2458]\n",
      "Epoch 42/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 207.28it/s, loss=0.3532]\n",
      "Epoch 43/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 264.13it/s, loss=0.2889]\n",
      "Epoch 44/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 234.96it/s, loss=0.2479]\n",
      "Epoch 45/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 247.72it/s, loss=0.3198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/250 | Avg Loss: 0.3055 | Accuracy: 0.8752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 217.09it/s, loss=0.3133]\n",
      "Epoch 47/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 243.23it/s, loss=0.2848]\n",
      "Epoch 48/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 270.69it/s, loss=0.2364]\n",
      "Epoch 49/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 223.40it/s, loss=0.2468]\n",
      "Epoch 50/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 239.41it/s, loss=0.2803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/250 | Avg Loss: 0.2992 | Accuracy: 0.8728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 230.99it/s, loss=0.2816]\n",
      "Epoch 52/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 261.83it/s, loss=0.1871]\n",
      "Epoch 53/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 245.76it/s, loss=0.2066]\n",
      "Epoch 54/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 156.67it/s, loss=0.2955]\n",
      "Epoch 55/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 261.07it/s, loss=0.3253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/250 | Avg Loss: 0.2924 | Accuracy: 0.8796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 205.09it/s, loss=0.3034]\n",
      "Epoch 57/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 226.56it/s, loss=0.2602]\n",
      "Epoch 58/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 196.69it/s, loss=0.3574]\n",
      "Epoch 59/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 166.44it/s, loss=0.2842]\n",
      "Epoch 60/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 251.37it/s, loss=0.3792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/250 | Avg Loss: 0.2863 | Accuracy: 0.8828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 234.02it/s, loss=0.1837]\n",
      "Epoch 62/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 230.32it/s, loss=0.1896]\n",
      "Epoch 63/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 262.36it/s, loss=0.2632]\n",
      "Epoch 64/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 216.23it/s, loss=0.2794]\n",
      "Epoch 65/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 257.41it/s, loss=0.2460]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/250 | Avg Loss: 0.2802 | Accuracy: 0.8876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 209.08it/s, loss=0.2203]\n",
      "Epoch 67/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 206.79it/s, loss=0.2772]\n",
      "Epoch 68/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 216.64it/s, loss=0.2815]\n",
      "Epoch 69/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 212.53it/s, loss=0.2594]\n",
      "Epoch 70/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 251.12it/s, loss=0.2898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/250 | Avg Loss: 0.2743 | Accuracy: 0.8896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 133.91it/s, loss=0.1842]\n",
      "Epoch 72/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 243.77it/s, loss=0.2874]\n",
      "Epoch 73/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 247.43it/s, loss=0.2471]\n",
      "Epoch 74/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 233.06it/s, loss=0.2427]\n",
      "Epoch 75/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 241.08it/s, loss=0.3260]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/250 | Avg Loss: 0.2685 | Accuracy: 0.8956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 202.07it/s, loss=0.2845]\n",
      "Epoch 77/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 220.27it/s, loss=0.3482]\n",
      "Epoch 78/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 211.35it/s, loss=0.2103]\n",
      "Epoch 79/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 178.53it/s, loss=0.2982]\n",
      "Epoch 80/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 211.77it/s, loss=0.2859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/250 | Avg Loss: 0.2629 | Accuracy: 0.8956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 212.55it/s, loss=0.2836]\n",
      "Epoch 82/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 223.53it/s, loss=0.2286]\n",
      "Epoch 83/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 224.51it/s, loss=0.2837]\n",
      "Epoch 84/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 279.39it/s, loss=0.3053]\n",
      "Epoch 85/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 233.85it/s, loss=0.2849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/250 | Avg Loss: 0.2573 | Accuracy: 0.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 212.30it/s, loss=0.2878]\n",
      "Epoch 87/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 233.47it/s, loss=0.2008]\n",
      "Epoch 88/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 210.18it/s, loss=0.2962]\n",
      "Epoch 89/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 158.12it/s, loss=0.2645]\n",
      "Epoch 90/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 214.61it/s, loss=0.2825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/250 | Avg Loss: 0.2520 | Accuracy: 0.9024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 184.35it/s, loss=0.2498]\n",
      "Epoch 92/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 224.34it/s, loss=0.2163]\n",
      "Epoch 93/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 244.83it/s, loss=0.3193]\n",
      "Epoch 94/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 216.67it/s, loss=0.2842]\n",
      "Epoch 95/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 189.60it/s, loss=0.2761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/250 | Avg Loss: 0.2463 | Accuracy: 0.9052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 196.83it/s, loss=0.2860]\n",
      "Epoch 97/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 210.70it/s, loss=0.2048]\n",
      "Epoch 98/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 163.99it/s, loss=0.2593]\n",
      "Epoch 99/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 224.96it/s, loss=0.1926]\n",
      "Epoch 100/250 (LR: 0.01): 100%|██████████| 20/20 [00:00<00:00, 239.38it/s, loss=0.1725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/250 | Avg Loss: 0.2413 | Accuracy: 0.9088\n",
      "\n",
      "Learning rate decayed to 0.005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 195.39it/s, loss=0.1869]\n",
      "Epoch 102/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 210.74it/s, loss=0.1936]\n",
      "Epoch 103/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 226.33it/s, loss=0.2099]\n",
      "Epoch 104/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 211.26it/s, loss=0.2357]\n",
      "Epoch 105/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 119.63it/s, loss=0.2198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/250 | Avg Loss: 0.2377 | Accuracy: 0.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 192.64it/s, loss=0.2682]\n",
      "Epoch 107/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 223.04it/s, loss=0.1456]\n",
      "Epoch 108/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 244.07it/s, loss=0.2448]\n",
      "Epoch 109/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 191.66it/s, loss=0.1311]\n",
      "Epoch 110/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 219.73it/s, loss=0.2568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/250 | Avg Loss: 0.2352 | Accuracy: 0.9120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 205.93it/s, loss=0.2906]\n",
      "Epoch 112/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 204.70it/s, loss=0.2939]\n",
      "Epoch 113/250 (LR: 0.005): 100%|██████████| 20/20 [-1:59:58<00:00, -8.36it/s, loss=0.1444]\n",
      "Epoch 114/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 202.89it/s, loss=0.2713]\n",
      "Epoch 115/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 204.90it/s, loss=0.2248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/250 | Avg Loss: 0.2328 | Accuracy: 0.9140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 181.57it/s, loss=0.2248]\n",
      "Epoch 117/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 210.28it/s, loss=0.2150]\n",
      "Epoch 118/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 226.32it/s, loss=0.2847]\n",
      "Epoch 119/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 184.63it/s, loss=0.2716]\n",
      "Epoch 120/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 257.31it/s, loss=0.1931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/250 | Avg Loss: 0.2300 | Accuracy: 0.9156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 258.09it/s, loss=0.2659]\n",
      "Epoch 122/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 145.12it/s, loss=0.2231]\n",
      "Epoch 123/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 237.18it/s, loss=0.2764]\n",
      "Epoch 124/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 190.44it/s, loss=0.2858]\n",
      "Epoch 125/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 178.34it/s, loss=0.2098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/250 | Avg Loss: 0.2275 | Accuracy: 0.9192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 185.37it/s, loss=0.2497]\n",
      "Epoch 127/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 231.05it/s, loss=0.2617]\n",
      "Epoch 128/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 203.26it/s, loss=0.1911]\n",
      "Epoch 129/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 183.13it/s, loss=0.2231]\n",
      "Epoch 130/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 239.10it/s, loss=0.2310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/250 | Avg Loss: 0.2251 | Accuracy: 0.9168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 205.39it/s, loss=0.2063]\n",
      "Epoch 132/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 273.76it/s, loss=0.2278]\n",
      "Epoch 133/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 234.25it/s, loss=0.2033]\n",
      "Epoch 134/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 206.26it/s, loss=0.2438]\n",
      "Epoch 135/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 186.09it/s, loss=0.1893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/250 | Avg Loss: 0.2227 | Accuracy: 0.9196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 223.24it/s, loss=0.1938]\n",
      "Epoch 137/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 105.28it/s, loss=0.1485]\n",
      "Epoch 138/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 251.93it/s, loss=0.1524]\n",
      "Epoch 139/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 153.67it/s, loss=0.1543]\n",
      "Epoch 140/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 238.26it/s, loss=0.3078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/250 | Avg Loss: 0.2202 | Accuracy: 0.9208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 225.62it/s, loss=0.1767]\n",
      "Epoch 142/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 226.10it/s, loss=0.1872]\n",
      "Epoch 143/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 114.65it/s, loss=0.1526]\n",
      "Epoch 144/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 194.49it/s, loss=0.2140]\n",
      "Epoch 145/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 170.12it/s, loss=0.1628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/250 | Avg Loss: 0.2177 | Accuracy: 0.9224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 209.48it/s, loss=0.2047]\n",
      "Epoch 147/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 227.95it/s, loss=0.1885]\n",
      "Epoch 148/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 215.12it/s, loss=0.1350]\n",
      "Epoch 149/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 201.26it/s, loss=0.3558]\n",
      "Epoch 150/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 207.96it/s, loss=0.2993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/250 | Avg Loss: 0.2153 | Accuracy: 0.9240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 161.68it/s, loss=0.2749]\n",
      "Epoch 152/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 188.11it/s, loss=0.1387]\n",
      "Epoch 153/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 223.82it/s, loss=0.2113]\n",
      "Epoch 154/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 143.99it/s, loss=0.2573]\n",
      "Epoch 155/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 201.37it/s, loss=0.2127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/250 | Avg Loss: 0.2130 | Accuracy: 0.9248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 189.75it/s, loss=0.2023]\n",
      "Epoch 157/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 211.43it/s, loss=0.2018]\n",
      "Epoch 158/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 239.09it/s, loss=0.1258]\n",
      "Epoch 159/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 211.16it/s, loss=0.2696]\n",
      "Epoch 160/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 216.45it/s, loss=0.2022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/250 | Avg Loss: 0.2105 | Accuracy: 0.9236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 187.65it/s, loss=0.2814]\n",
      "Epoch 162/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 255.06it/s, loss=0.1662]\n",
      "Epoch 163/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 141.37it/s, loss=0.2603]\n",
      "Epoch 164/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 194.19it/s, loss=0.3104]\n",
      "Epoch 165/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 225.97it/s, loss=0.1852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/250 | Avg Loss: 0.2083 | Accuracy: 0.9272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 216.54it/s, loss=0.1592]\n",
      "Epoch 167/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 227.21it/s, loss=0.2184]\n",
      "Epoch 168/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 203.51it/s, loss=0.2036]\n",
      "Epoch 169/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 180.86it/s, loss=0.1858]\n",
      "Epoch 170/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 195.62it/s, loss=0.1385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/250 | Avg Loss: 0.2059 | Accuracy: 0.9264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 151.78it/s, loss=0.2645]\n",
      "Epoch 172/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 243.60it/s, loss=0.2440]\n",
      "Epoch 173/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 221.52it/s, loss=0.1759]\n",
      "Epoch 174/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 221.99it/s, loss=0.1757]\n",
      "Epoch 175/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 227.84it/s, loss=0.2384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/250 | Avg Loss: 0.2036 | Accuracy: 0.9288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 197.38it/s, loss=0.1698]\n",
      "Epoch 177/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 200.23it/s, loss=0.2094]\n",
      "Epoch 178/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 233.85it/s, loss=0.1821]\n",
      "Epoch 179/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 221.79it/s, loss=0.2915]\n",
      "Epoch 180/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 232.14it/s, loss=0.2561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/250 | Avg Loss: 0.2012 | Accuracy: 0.9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 203.29it/s, loss=0.2985]\n",
      "Epoch 182/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 205.13it/s, loss=0.1814]\n",
      "Epoch 183/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 264.16it/s, loss=0.3397]\n",
      "Epoch 184/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 206.01it/s, loss=0.1473]\n",
      "Epoch 185/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 241.53it/s, loss=0.2204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/250 | Avg Loss: 0.1991 | Accuracy: 0.9312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 258.96it/s, loss=0.1224]\n",
      "Epoch 187/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 233.04it/s, loss=0.1878]\n",
      "Epoch 188/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 141.11it/s, loss=0.2847]\n",
      "Epoch 189/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 228.40it/s, loss=0.1270]\n",
      "Epoch 190/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 170.29it/s, loss=0.1567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/250 | Avg Loss: 0.1968 | Accuracy: 0.9320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 191/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 175.41it/s, loss=0.2000]\n",
      "Epoch 192/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 224.37it/s, loss=0.1669]\n",
      "Epoch 193/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 209.53it/s, loss=0.1722]\n",
      "Epoch 194/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 256.72it/s, loss=0.1942]\n",
      "Epoch 195/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 242.80it/s, loss=0.2048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/250 | Avg Loss: 0.1947 | Accuracy: 0.9332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 196/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 182.34it/s, loss=0.1582]\n",
      "Epoch 197/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 264.20it/s, loss=0.2320]\n",
      "Epoch 198/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 243.15it/s, loss=0.1405]\n",
      "Epoch 199/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 177.65it/s, loss=0.1670]\n",
      "Epoch 200/250 (LR: 0.005): 100%|██████████| 20/20 [00:00<00:00, 229.92it/s, loss=0.3136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/250 | Avg Loss: 0.1923 | Accuracy: 0.9340\n",
      "\n",
      "Learning rate decayed to 0.0025\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 201/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 200.75it/s, loss=0.2903]\n",
      "Epoch 202/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 223.84it/s, loss=0.1879]\n",
      "Epoch 203/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 219.83it/s, loss=0.1801]\n",
      "Epoch 204/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 157.05it/s, loss=0.1720]\n",
      "Epoch 205/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 241.67it/s, loss=0.2927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/250 | Avg Loss: 0.1908 | Accuracy: 0.9344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 206/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 233.61it/s, loss=0.1291]\n",
      "Epoch 207/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 208.10it/s, loss=0.1958]\n",
      "Epoch 208/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 226.70it/s, loss=0.2002]\n",
      "Epoch 209/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 188.20it/s, loss=0.1870]\n",
      "Epoch 210/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 204.06it/s, loss=0.1743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210/250 | Avg Loss: 0.1897 | Accuracy: 0.9356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 211/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 179.24it/s, loss=0.1852]\n",
      "Epoch 212/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 247.62it/s, loss=0.2082]\n",
      "Epoch 213/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 210.17it/s, loss=0.1704]\n",
      "Epoch 214/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 229.45it/s, loss=0.2048]\n",
      "Epoch 215/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 201.17it/s, loss=0.2182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/250 | Avg Loss: 0.1886 | Accuracy: 0.9368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 216/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 220.85it/s, loss=0.1888]\n",
      "Epoch 217/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 249.99it/s, loss=0.2592]\n",
      "Epoch 218/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 211.96it/s, loss=0.1460]\n",
      "Epoch 219/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 235.52it/s, loss=0.1752]\n",
      "Epoch 220/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 239.25it/s, loss=0.1863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/250 | Avg Loss: 0.1875 | Accuracy: 0.9376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 221/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 171.37it/s, loss=0.1546]\n",
      "Epoch 222/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 253.03it/s, loss=0.2340]\n",
      "Epoch 223/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 240.41it/s, loss=0.2122]\n",
      "Epoch 224/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 241.48it/s, loss=0.2142]\n",
      "Epoch 225/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 242.29it/s, loss=0.1896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/250 | Avg Loss: 0.1864 | Accuracy: 0.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 226/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 198.53it/s, loss=0.1876]\n",
      "Epoch 227/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 232.05it/s, loss=0.1891]\n",
      "Epoch 228/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 233.44it/s, loss=0.1892]\n",
      "Epoch 229/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 211.08it/s, loss=0.1259]\n",
      "Epoch 230/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 217.24it/s, loss=0.2236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/250 | Avg Loss: 0.1854 | Accuracy: 0.9384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 231/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 191.54it/s, loss=0.2166]\n",
      "Epoch 232/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 285.32it/s, loss=0.1842]\n",
      "Epoch 233/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 243.86it/s, loss=0.2001]\n",
      "Epoch 234/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 256.07it/s, loss=0.2016]\n",
      "Epoch 235/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 204.35it/s, loss=0.1692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/250 | Avg Loss: 0.1844 | Accuracy: 0.9392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 236/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 191.72it/s, loss=0.1833]\n",
      "Epoch 237/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 236.73it/s, loss=0.1398]\n",
      "Epoch 238/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 202.06it/s, loss=0.2237]\n",
      "Epoch 239/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 195.38it/s, loss=0.1567]\n",
      "Epoch 240/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 232.30it/s, loss=0.2034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/250 | Avg Loss: 0.1834 | Accuracy: 0.9392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 241/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 197.14it/s, loss=0.1911]\n",
      "Epoch 242/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 270.70it/s, loss=0.2402]\n",
      "Epoch 243/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 216.05it/s, loss=0.1597]\n",
      "Epoch 244/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 225.89it/s, loss=0.1068]\n",
      "Epoch 245/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 198.73it/s, loss=0.2199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245/250 | Avg Loss: 0.1822 | Accuracy: 0.9404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 246/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 202.86it/s, loss=0.1411]\n",
      "Epoch 247/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 212.06it/s, loss=0.2452]\n",
      "Epoch 248/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 174.33it/s, loss=0.2307]\n",
      "Epoch 249/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 230.40it/s, loss=0.1682]\n",
      "Epoch 250/250 (LR: 0.0025): 100%|██████████| 20/20 [00:00<00:00, 202.07it/s, loss=0.1481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/250 | Avg Loss: 0.1812 | Accuracy: 0.9416\n",
      "\n",
      "Training complete. Saving model...\n",
      "Model saved to runs/20251013-175009/final_model.npz\n",
      "Model saved. Now preparing final summary plot...\n",
      "Generating final summary plot...\n",
      "Plot saved to file: runs/20251013-175009/final_summary.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1526/1232716160.py:244: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "class BorderDataset:\n",
    "    \"\"\"\n",
    "    Loads and processes the 50x50 border image, converting it into a dataset\n",
    "    of normalized coordinates and binary labels.\n",
    "    \"\"\"\n",
    "    NETHERLANDS_ORANGE = np.array([255, 165, 0])\n",
    "    BELGIUM_PURPLE = np.array([138, 43, 226])\n",
    "\n",
    "    def __init__(self, image_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading and processing the image.\n",
    "        \"\"\"\n",
    "        self.image_path = image_path\n",
    "        self.binary_mask = self._create_binary_mask()\n",
    "        self.height, self.width = self.binary_mask.shape\n",
    "        self.pixels = self._prepare_pixels()\n",
    "        print(f\"Dataset created from '{self.image_path}' with {len(self.pixels)} points.\")\n",
    "\n",
    "    def _create_binary_mask(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Opens the image and converts it to a 0-1 binary mask.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            img = Image.open(self.image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image file not found. Creating a dummy image at '{self.image_path}'.\")\n",
    "            img = self._create_dummy_image()\n",
    "        \n",
    "        img_array = np.array(img)\n",
    "        dist_to_orange = np.linalg.norm(img_array - self.NETHERLANDS_ORANGE, axis=2)\n",
    "        dist_to_purple = np.linalg.norm(img_array - self.BELGIUM_PURPLE, axis=2)\n",
    "        mask = (dist_to_purple < dist_to_orange).astype(int)\n",
    "        return mask\n",
    "\n",
    "    def _create_dummy_image(self) -> Image:\n",
    "        \"\"\"\n",
    "        Creates and saves a fallback 50x50 dummy image.\n",
    "        \"\"\"\n",
    "        dummy_array = np.zeros((50, 50, 3), dtype=np.uint8)\n",
    "        dummy_array[:, :] = self.NETHERLANDS_ORANGE\n",
    "        dummy_array[10:25, 10:40] = self.BELGIUM_PURPLE\n",
    "        dummy_array[30:45, 15:35] = self.BELGIUM_PURPLE\n",
    "        img = Image.fromarray(dummy_array)\n",
    "        \n",
    "        directory = os.path.dirname(self.image_path)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        img.save(self.image_path)\n",
    "        return img\n",
    "\n",
    "    def _prepare_pixels(self) -> list:\n",
    "        \"\"\"\n",
    "        Generates a list of ((x, y), label) tuples for each pixel.\n",
    "        \"\"\"\n",
    "        pixel_data = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                normalized_x = x / (self.width - 1)\n",
    "                normalized_y = y / (self.height - 1)\n",
    "                label = self.binary_mask[y, x]\n",
    "                pixel_data.append(((normalized_x, normalized_y), label))\n",
    "        return pixel_data\n",
    "\n",
    "    def get_shuffled_data(self) -> list:\n",
    "        \"\"\"\n",
    "        Returns a randomly shuffled copy of the dataset.\n",
    "        \"\"\"\n",
    "        shuffled_pixels = self.pixels.copy()\n",
    "        np.random.shuffle(shuffled_pixels)\n",
    "        return shuffled_pixels\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"Rectified Linear Unit activation function.\"\"\"\n",
    "    def forward(self, x): return np.maximum(0, x)\n",
    "    def backward(self, x): return (x > 0).astype(float)\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    def forward(self, x): return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    def backward(self, x):\n",
    "        s = self.forward(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "class Linear:\n",
    "    \"\"\"A single fully-connected neural network layer.\"\"\"\n",
    "    def __init__(self, input_width, output_width, activation):\n",
    "        self.weights = np.random.randn(input_width, output_width) * np.sqrt(2. / input_width)\n",
    "        self.biases = np.zeros((1, output_width))\n",
    "        self.activation = activation()\n",
    "        self.input_data, self.z = None, None\n",
    "        self.grad_weights = np.zeros_like(self.weights)\n",
    "        self.grad_biases = np.zeros_like(self.biases)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input_data = x\n",
    "        self.z = np.dot(x, self.weights) + self.biases\n",
    "        return self.activation.forward(self.z)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        delta = grad_output * self.activation.backward(self.z)\n",
    "        self.grad_weights += np.dot(self.input_data.T, delta)\n",
    "        self.grad_biases += np.sum(delta, axis=0, keepdims=True)\n",
    "        return np.dot(delta, self.weights.T)\n",
    "\n",
    "class BCE:\n",
    "    \"\"\"Binary Cross-Entropy loss function.\"\"\"\n",
    "    def loss(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred)) / y_true.size\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    The main neural network model class.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, loss_function='bce'):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = BCE()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_loss):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_loss = layer.backward(grad_loss)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.loss_fn.loss(y, y_pred)\n",
    "        self.backward(self.loss_fn.backward(y, y_pred))\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.grad_weights.fill(0)\n",
    "            layer.grad_biases.fill(0)\n",
    "\n",
    "    def update(self, learning_rate, grad_clip_value=1.0):\n",
    "        for layer in self.layers:\n",
    "            np.clip(layer.grad_weights, -grad_clip_value, grad_clip_value, out=layer.grad_weights)\n",
    "            np.clip(layer.grad_biases, -grad_clip_value, grad_clip_value, out=layer.grad_biases)\n",
    "            layer.weights -= learning_rate * layer.grad_weights\n",
    "            layer.biases -= learning_rate * layer.grad_biases\n",
    "        self.zero_grad()\n",
    "\n",
    "    def save_to(self, path):\n",
    "        params = {f'w_{i}': l.weights for i, l in enumerate(self.layers)}\n",
    "        params.update({f'b_{i}': l.biases for i, l in enumerate(self.layers)})\n",
    "        np.savez(path, **params)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_from(self, path):\n",
    "        data = np.load(path)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if layer.weights.shape != data[f'w_{i}'].shape or layer.biases.shape != data[f'b_{i}'].shape:\n",
    "                raise ValueError(f\"Architecture mismatch in layer {i}.\")\n",
    "            layer.weights, layer.biases = data[f'w_{i}'], data[f'b_{i}']\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "def positional_encoding(coords, num_frequencies):\n",
    "    \"\"\"\n",
    "    Encodes coordinates to a higher dimension using sine and cosine functions.\n",
    "    \"\"\"\n",
    "    frequencies = 2**np.linspace(0, num_frequencies-1, num_frequencies)\n",
    "    encoded_coords = []\n",
    "    for coord in coords.T: \n",
    "        sines = np.sin(2 * np.pi * coord[:, None] * frequencies)\n",
    "        cosines = np.cos(2 * np.pi * coord[:, None] * frequencies)\n",
    "        encoded_coords.extend([sines, cosines])\n",
    "    return np.hstack(encoded_coords)\n",
    "\n",
    "def plot_final_summary(history, model, dataset, run_folder, username, num_frequencies):\n",
    "    \"\"\"\n",
    "    Generates and saves a single, consolidated plot.\n",
    "    \"\"\"\n",
    "    print(\"Generating final summary plot...\")\n",
    "    plt.close('all') \n",
    "    \n",
    "    h, w = dataset.height, dataset.width\n",
    "    x_coords = np.linspace(0, 1, w)\n",
    "    y_coords = np.linspace(0, 1, h)\n",
    "    grid_x, grid_y = np.meshgrid(x_coords, y_coords)\n",
    "    all_coords_raw = np.vstack([grid_x.ravel(), grid_y.ravel()]).T\n",
    "    \n",
    "    all_coords_encoded = positional_encoding(all_coords_raw, num_frequencies)\n",
    "    \n",
    "    predictions_raw = model.predict(all_coords_encoded)\n",
    "    prediction_map = (predictions_raw > 0.5).astype(int).reshape((h, w))\n",
    "    ground_truth_map = dataset.binary_mask\n",
    "    error_map = np.abs(ground_truth_map - prediction_map)\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(2, 3, height_ratios=[1, 1.2])\n",
    "    fig.suptitle(f'Training and Prediction Summary - {username}', fontsize=20)\n",
    "\n",
    "    ax_loss = fig.add_subplot(gs[0, 0])\n",
    "    ax_loss.plot(history['loss'])\n",
    "    ax_loss.set_title(\"Average Loss vs. Epochs\")\n",
    "    ax_loss.set_xlabel(\"Epoch\"); ax_loss.set_ylabel(\"Loss\"); ax_loss.grid(True)\n",
    "\n",
    "    ax_acc = fig.add_subplot(gs[0, 1])\n",
    "    ax_acc.plot(history['accuracy'])\n",
    "    ax_acc.set_title(\"Accuracy vs. Epochs\")\n",
    "    ax_acc.set_xlabel(\"Epoch\"); ax_acc.set_ylabel(\"Accuracy\"); ax_acc.grid(True)\n",
    "    \n",
    "    ax_stats = fig.add_subplot(gs[0, 2])\n",
    "    final_acc = history['accuracy'][-1]\n",
    "    final_loss = history['loss'][-1]\n",
    "    total_epochs = len(history['loss'])\n",
    "    stats_text = (f\"Final Accuracy: {final_acc:.4f}\\n\\n\"\n",
    "                  f\"Final Loss: {final_loss:.4f}\\n\\n\"\n",
    "                  f\"Total Epochs: {total_epochs}\")\n",
    "    ax_stats.text(0.5, 0.5, stats_text, ha='center', va='center', fontsize=14)\n",
    "    ax_stats.axis('off')\n",
    "    ax_stats.set_title(\"Final Stats\")\n",
    "\n",
    "    ax_gt = fig.add_subplot(gs[1, 0])\n",
    "    ax_gt.imshow(ground_truth_map, cmap='viridis', interpolation='nearest')\n",
    "    ax_gt.set_title(\"Ground Truth\"); ax_gt.axis('off')\n",
    "\n",
    "    ax_pred = fig.add_subplot(gs[1, 1])\n",
    "    ax_pred.imshow(prediction_map, cmap='viridis', interpolation='nearest')\n",
    "    ax_pred.set_title(\"Model Prediction\"); ax_pred.axis('off')\n",
    "\n",
    "    ax_err = fig.add_subplot(gs[1, 2])\n",
    "    ax_err.imshow(error_map, cmap='hot', interpolation='nearest')\n",
    "    ax_err.set_title(\"Error Map\"); ax_err.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
    "    \n",
    "    plot_path = os.path.join(run_folder, \"final_summary.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"Plot saved to file: {plot_path}\")\n",
    "    \n",
    "    plt.show() \n",
    "    plt.close(fig) \n",
    "\n",
    "\n",
    "def training_procedure(model, dataset, epochs, batch_size, initial_learning_rate, patience, username, num_frequencies, is_silent=False):\n",
    "    \"\"\"\n",
    "    Handles the main training loop for all tasks.\n",
    "    \"\"\"\n",
    "    data = dataset if isinstance(dataset, list) else dataset.get_shuffled_data()\n",
    "    X_raw = np.array([item[0] for item in data])\n",
    "    Y = np.array([item[1] for item in data]).reshape(-1, 1)\n",
    "    \n",
    "    X_encoded = positional_encoding(X_raw, num_frequencies)\n",
    "    n_samples = len(data)\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    run_folder = f\"runs/{timestamp}\"\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    if not is_silent:\n",
    "        print(f\"Saving results to {run_folder}\")\n",
    "\n",
    "    learning_rate = initial_learning_rate\n",
    "    for epoch in range(epochs):\n",
    "        if epoch > 0 and epoch % 100 == 0:\n",
    "            learning_rate /= 2\n",
    "            if not is_silent:\n",
    "                print(f\"\\nLearning rate decayed to {learning_rate}\\n\")\n",
    "\n",
    "        epoch_loss = 0\n",
    "        permutation = np.random.permutation(n_samples)\n",
    "        X_shuffled, Y_shuffled = X_encoded[permutation], Y[permutation]\n",
    "        \n",
    "        batch_iterator = range(0, n_samples, batch_size)\n",
    "        if not is_silent:\n",
    "            batch_iterator = tqdm(batch_iterator, desc=f\"Epoch {epoch+1}/{epochs} (LR: {learning_rate})\")\n",
    "\n",
    "        for i in batch_iterator:\n",
    "            batch_X = X_shuffled[i:i+batch_size]\n",
    "            batch_Y = Y_shuffled[i:i+batch_size]\n",
    "            loss = model.train(batch_X, batch_Y)\n",
    "            model.update(learning_rate)\n",
    "            epoch_loss += loss * batch_X.shape[0]\n",
    "            if not is_silent:\n",
    "                batch_iterator.set_postfix(loss=f\"{loss:.4f}\")\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / n_samples\n",
    "        \n",
    "        if np.isnan(avg_epoch_loss):\n",
    "            print(\"\\nCRITICAL ERROR: Loss has become NaN. Training cannot continue.\")\n",
    "            return None\n",
    "\n",
    "        y_pred_full = model.predict(X_encoded)\n",
    "        predictions = (y_pred_full > 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        \n",
    "        history['loss'].append(avg_epoch_loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "        \n",
    "        if not is_silent:\n",
    "            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | Avg Loss: {avg_epoch_loss:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        if epoch >= patience:\n",
    "            if history['loss'][-1] >= (1.0 - 0.005) * history['loss'][-1 - patience]:\n",
    "                if not is_silent:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "                    if (epoch + 1) % 5 != 0:\n",
    "                        print(f\"Final Epoch {epoch+1}/{epochs} | Avg Loss: {avg_epoch_loss:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "                break\n",
    "    \n",
    "    if not is_silent:\n",
    "        print(\"\\nTraining complete. Saving model...\")\n",
    "        model.save_to(os.path.join(run_folder, \"final_model.npz\"))\n",
    "        print(\"Model saved. Now preparing final summary plot...\")\n",
    "        if not isinstance(dataset, list):\n",
    "            plot_final_summary(history, model, dataset, run_folder, username, num_frequencies)\n",
    "\n",
    "    return history\n",
    "\n",
    "# This code will run when you execute the cell.\n",
    "# It creates the border_dataset object in the main scope of the notebook,\n",
    "# so it will be available for the next cell to use.\n",
    "\n",
    "print(\"Executing Section 1.1: Dataset Creation\")\n",
    "image_file = \"Dataset/border.png\"\n",
    "border_dataset = BorderDataset(image_file)\n",
    "\n",
    "print(\"\\nExecuting Section 1.2: Main Model Training\")\n",
    "\n",
    "num_frequencies = 10\n",
    "input_dim = 4 * num_frequencies \n",
    "\n",
    "model = Model([\n",
    "    Linear(input_dim, 256, ReLU),\n",
    "    Linear(256, 256, ReLU),\n",
    "    Linear(256, 1, Sigmoid)\n",
    "])\n",
    "\n",
    "print(\"Model created. Starting training on border map...\")\n",
    "# The history object is captured here to prevent the notebook from auto-printing it.\n",
    "history = training_procedure(\n",
    "    model=model,\n",
    "    dataset=border_dataset,\n",
    "    epochs=250, \n",
    "    batch_size=128,\n",
    "    initial_learning_rate=0.01,\n",
    "    patience=100,\n",
    "    username=\"sudershan.sarraf\",\n",
    "    num_frequencies=num_frequencies,\n",
    "    is_silent=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e339be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing Sanity Check: The XOR Problem\n",
      "Training model on XOR data...\n",
      "XOR test final accuracy: 1.0000\n",
      "XOR Test PASSED: Model achieved 100% accuracy.\n",
      "\n",
      "Executing Sanity Check: Gradient Check\n",
      "Comparing analytical and numerical gradients...\n",
      "Layer 0 Relative Error: 2.0770530088677715e-11\n",
      "Layer 1 Relative Error: 1.1733377223073913e-11\n",
      "Gradient Check PASSED.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# This script is fully self-contained and can be run in a new cell.\n",
    "# It includes all necessary class and function definitions.\n",
    "\n",
    "class BorderDataset:\n",
    "    \"\"\"\n",
    "    Loads and processes the 50x50 border image, converting it into a dataset\n",
    "    of normalized coordinates and binary labels.\n",
    "    \"\"\"\n",
    "    NETHERLANDS_ORANGE = np.array([255, 165, 0])\n",
    "    BELGIUM_PURPLE = np.array([138, 43, 226])\n",
    "\n",
    "    def __init__(self, image_path: str):\n",
    "        self.image_path = image_path\n",
    "        self.binary_mask = self._create_binary_mask()\n",
    "        self.height, self.width = self.binary_mask.shape\n",
    "        self.pixels = self._prepare_pixels()\n",
    "        print(f\"Dataset created from '{self.image_path}' with {len(self.pixels)} points.\")\n",
    "\n",
    "    def _create_binary_mask(self) -> np.ndarray:\n",
    "        try:\n",
    "            img = Image.open(self.image_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image file not found. Creating a dummy image at '{self.image_path}'.\")\n",
    "            img = self._create_dummy_image()\n",
    "        \n",
    "        img_array = np.array(img)\n",
    "        dist_to_orange = np.linalg.norm(img_array - self.NETHERLANDS_ORANGE, axis=2)\n",
    "        dist_to_purple = np.linalg.norm(img_array - self.BELGIUM_PURPLE, axis=2)\n",
    "        mask = (dist_to_purple < dist_to_orange).astype(int)\n",
    "        return mask\n",
    "\n",
    "    def _create_dummy_image(self) -> Image:\n",
    "        dummy_array = np.zeros((50, 50, 3), dtype=np.uint8)\n",
    "        dummy_array[:, :] = self.NETHERLANDS_ORANGE\n",
    "        dummy_array[10:25, 10:40] = self.BELGIUM_PURPLE\n",
    "        dummy_array[30:45, 15:35] = self.BELGIUM_PURPLE\n",
    "        img = Image.fromarray(dummy_array)\n",
    "        \n",
    "        directory = os.path.dirname(self.image_path)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        img.save(self.image_path)\n",
    "        return img\n",
    "\n",
    "    def _prepare_pixels(self) -> list:\n",
    "        pixel_data = []\n",
    "        for y in range(self.height):\n",
    "            for x in range(self.width):\n",
    "                normalized_x = x / (self.width - 1)\n",
    "                normalized_y = y / (self.height - 1)\n",
    "                label = self.binary_mask[y, x]\n",
    "                pixel_data.append(((normalized_x, normalized_y), label))\n",
    "        return pixel_data\n",
    "\n",
    "    def get_shuffled_data(self) -> list:\n",
    "        shuffled_pixels = self.pixels.copy()\n",
    "        np.random.shuffle(shuffled_pixels)\n",
    "        return shuffled_pixels\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"Rectified Linear Unit activation function.\"\"\"\n",
    "    def forward(self, x): return np.maximum(0, x)\n",
    "    def backward(self, x): return (x > 0).astype(float)\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    def forward(self, x): return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    def backward(self, x):\n",
    "        s = self.forward(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "class Linear:\n",
    "    \"\"\"A single fully-connected neural network layer.\"\"\"\n",
    "    def __init__(self, input_width, output_width, activation):\n",
    "        self.weights = np.random.randn(input_width, output_width) * np.sqrt(2. / input_width)\n",
    "        self.biases = np.zeros((1, output_width))\n",
    "        self.activation = activation()\n",
    "        self.input_data, self.z = None, None\n",
    "        self.grad_weights = np.zeros_like(self.weights)\n",
    "        self.grad_biases = np.zeros_like(self.biases)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input_data = x\n",
    "        self.z = np.dot(x, self.weights) + self.biases\n",
    "        return self.activation.forward(self.z)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        delta = grad_output * self.activation.backward(self.z)\n",
    "        self.grad_weights += np.dot(self.input_data.T, delta)\n",
    "        self.grad_biases += np.sum(delta, axis=0, keepdims=True)\n",
    "        return np.dot(delta, self.weights.T)\n",
    "\n",
    "class BCE:\n",
    "    \"\"\"Binary Cross-Entropy loss function.\"\"\"\n",
    "    def loss(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred)) / y_true.size\n",
    "\n",
    "class Model:\n",
    "    \"\"\"The main neural network model class.\"\"\"\n",
    "    def __init__(self, layers, loss_function='bce'):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = BCE()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_loss):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_loss = layer.backward(grad_loss)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.loss_fn.loss(y, y_pred)\n",
    "        self.backward(self.loss_fn.backward(y, y_pred))\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.grad_weights.fill(0)\n",
    "            layer.grad_biases.fill(0)\n",
    "\n",
    "    def update(self, learning_rate, grad_clip_value=1.0):\n",
    "        for layer in self.layers:\n",
    "            np.clip(layer.grad_weights, -grad_clip_value, grad_clip_value, out=layer.grad_weights)\n",
    "            np.clip(layer.grad_biases, -grad_clip_value, grad_clip_value, out=layer.grad_biases)\n",
    "            layer.weights -= learning_rate * layer.grad_weights\n",
    "            layer.biases -= learning_rate * layer.grad_biases\n",
    "        self.zero_grad()\n",
    "\n",
    "def positional_encoding(coords, num_frequencies):\n",
    "    \"\"\"Encodes coordinates to a higher dimension using sine and cosine functions.\"\"\"\n",
    "    frequencies = 2**np.linspace(0, num_frequencies-1, num_frequencies)\n",
    "    encoded_coords = []\n",
    "    for coord in coords.T: \n",
    "        sines = np.sin(2 * np.pi * coord[:, None] * frequencies)\n",
    "        cosines = np.cos(2 * np.pi * coord[:, None] * frequencies)\n",
    "        encoded_coords.extend([sines, cosines])\n",
    "    return np.hstack(encoded_coords)\n",
    "\n",
    "def training_procedure(model, dataset, epochs, batch_size, initial_learning_rate, patience, username, num_frequencies, is_silent=False):\n",
    "    \"\"\"Handles the main training loop for all tasks.\"\"\"\n",
    "    data = dataset if isinstance(dataset, list) else dataset.get_shuffled_data()\n",
    "    X_raw = np.array([item[0] for item in data])\n",
    "    Y = np.array([item[1] for item in data]).reshape(-1, 1)\n",
    "    \n",
    "    X_encoded = positional_encoding(X_raw, num_frequencies)\n",
    "    n_samples = len(data)\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        permutation = np.random.permutation(n_samples)\n",
    "        X_shuffled, Y_shuffled = X_encoded[permutation], Y[permutation]\n",
    "        \n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch_X = X_shuffled[i:i+batch_size]\n",
    "            batch_Y = Y_shuffled[i:i+batch_size]\n",
    "            loss = model.train(batch_X, batch_Y)\n",
    "            model.update(initial_learning_rate)\n",
    "            epoch_loss += loss * batch_X.shape[0]\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / n_samples\n",
    "        \n",
    "        if np.isnan(avg_epoch_loss):\n",
    "            print(\"\\nCRITICAL ERROR: Loss has become NaN.\")\n",
    "            return None\n",
    "\n",
    "        y_pred_full = model.predict(X_encoded)\n",
    "        predictions = (y_pred_full > 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        \n",
    "        history['loss'].append(avg_epoch_loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "\n",
    "        if epoch >= patience:\n",
    "            if history['loss'][-1] >= (1.0 - 0.005) * history['loss'][-1 - patience]:\n",
    "                if not is_silent:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "                break\n",
    "    return history\n",
    "\n",
    "def xor_training_procedure(model, dataset, epochs, batch_size, learning_rate, patience):\n",
    "    \"\"\"A simplified training loop for the XOR test without positional encoding.\"\"\"\n",
    "    X = np.array([item[0] for item in dataset])\n",
    "    Y = np.array([item[1] for item in dataset]).reshape(-1, 1)\n",
    "    n_samples = len(dataset)\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        permutation = np.random.permutation(n_samples)\n",
    "        X_shuffled, Y_shuffled = X[permutation], Y[permutation]\n",
    "        \n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch_X = X_shuffled[i:i+batch_size]\n",
    "            batch_Y = Y_shuffled[i:i+batch_size]\n",
    "            loss = model.train(batch_X, batch_Y)\n",
    "            model.update(learning_rate)\n",
    "            epoch_loss += loss * batch_X.shape[0]\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / n_samples\n",
    "        \n",
    "        if np.isnan(avg_epoch_loss):\n",
    "            return None\n",
    "\n",
    "        y_pred_full = model.predict(X)\n",
    "        predictions = (y_pred_full > 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == Y)\n",
    "        \n",
    "        history['loss'].append(avg_epoch_loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "\n",
    "        if accuracy == 1.0 and epoch > 100: # Stop once 100% is reached\n",
    "            break\n",
    "            \n",
    "    return history\n",
    "\n",
    "# Sanity Check: XOR Test\n",
    "def execute_xor_test():\n",
    "    \"\"\"Performs the XOR sanity check to verify the MLP implementation.\"\"\"\n",
    "    print(\"\\nExecuting Sanity Check: The XOR Problem\")\n",
    "    xor_data = [((0, 0), 0), ((0, 1), 1), ((1, 0), 1), ((1, 1), 0)]\n",
    "    \n",
    "    # The model for XOR takes 2 raw inputs, not encoded ones.\n",
    "    model = Model([\n",
    "        Linear(2, 8, ReLU),\n",
    "        Linear(8, 1, Sigmoid)\n",
    "    ])\n",
    "    \n",
    "    print(\"Training model on XOR data...\")\n",
    "    # Use the dedicated XOR training loop\n",
    "    history = xor_training_procedure(\n",
    "        model=model, dataset=xor_data, epochs=3000, batch_size=4,\n",
    "        learning_rate=0.1, patience=200\n",
    "    )\n",
    "    \n",
    "    if history:\n",
    "        final_accuracy = history['accuracy'][-1]\n",
    "        print(f\"XOR test final accuracy: {final_accuracy:.4f}\")\n",
    "        if final_accuracy == 1.0:\n",
    "            print(\"XOR Test PASSED: Model achieved 100% accuracy.\")\n",
    "        else:\n",
    "            print(\"XOR Test FAILED: Model did not achieve 100% accuracy.\")\n",
    "\n",
    "# Sanity Check: Gradient Check\n",
    "def execute_gradient_check():\n",
    "    \"\"\"Performs a gradient check by comparing analytical and numerical gradients.\"\"\"\n",
    "    print(\"\\nExecuting Sanity Check: Gradient Check\")\n",
    "    \n",
    "    x = np.array([[0.5, 0.5]])\n",
    "    y = np.array([[1]])\n",
    "    num_frequencies = 4\n",
    "    input_dim = 4 * num_frequencies\n",
    "    x_encoded = positional_encoding(x, num_frequencies)\n",
    "\n",
    "    model = Model([Linear(input_dim, 2, ReLU), Linear(2, 1, Sigmoid)])\n",
    "    \n",
    "    model.train(x_encoded, y)\n",
    "    analytical_grads = [np.copy(l.grad_weights) for l in model.layers]\n",
    "    model.zero_grad()\n",
    "\n",
    "    epsilon = 1e-5\n",
    "    numerical_grads = [np.zeros_like(l.weights) for l in model.layers]\n",
    "    \n",
    "    for l_idx, layer in enumerate(model.layers):\n",
    "        for i in range(layer.weights.shape[0]):\n",
    "            for j in range(layer.weights.shape[1]):\n",
    "                original_weight = layer.weights[i, j]\n",
    "                \n",
    "                layer.weights[i, j] = original_weight + epsilon\n",
    "                loss_plus = model.loss_fn.loss(y, model.forward(x_encoded))\n",
    "                \n",
    "                layer.weights[i, j] = original_weight - epsilon\n",
    "                loss_minus = model.loss_fn.loss(y, model.forward(x_encoded))\n",
    "                \n",
    "                layer.weights[i, j] = original_weight\n",
    "                \n",
    "                numerical_grads[l_idx][i, j] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "    total_relative_error = 0\n",
    "    print(\"Comparing analytical and numerical gradients...\")\n",
    "    for l_idx in range(len(model.layers)):\n",
    "        numerator = np.linalg.norm(analytical_grads[l_idx] - numerical_grads[l_idx])\n",
    "        denominator = np.linalg.norm(analytical_grads[l_idx]) + np.linalg.norm(numerical_grads[l_idx])\n",
    "        relative_error = numerator / denominator if denominator > 1e-8 else 0\n",
    "        total_relative_error += relative_error\n",
    "        print(f\"Layer {l_idx} Relative Error: {relative_error}\")\n",
    "\n",
    "    if total_relative_error < 1e-6:\n",
    "        print(\"Gradient Check PASSED.\")\n",
    "    else:\n",
    "        print(\"Gradient Check FAILED.\")\n",
    "\n",
    "# Map Prediction and Analysis\n",
    "def execute_analysis(dataset):\n",
    "    \"\"\"Trains models with varying architectures and plots the results.\"\"\"\n",
    "    print(\"\\nExecuting Map Prediction and Analysis\")\n",
    "    num_frequencies = 10\n",
    "    input_dim = 4 * num_frequencies\n",
    "\n",
    "    print(\"\\nAnalyzing effect of network depth...\")\n",
    "    depths = [2, 3, 4, 5]\n",
    "    width = 128\n",
    "    depth_accuracies = []\n",
    "    for depth in depths:\n",
    "        print(f\"Training model with depth {depth} and width {width}...\")\n",
    "        layers = [Linear(input_dim, width, ReLU)]\n",
    "        for _ in range(depth - 1):\n",
    "            layers.append(Linear(width, width, ReLU))\n",
    "        layers.append(Linear(width, 1, Sigmoid))\n",
    "        model = Model(layers)\n",
    "        \n",
    "        history = training_procedure(model, dataset, epochs=150, batch_size=128, initial_learning_rate=0.01, patience=20, username=f\"Depth_{depth}\", num_frequencies=num_frequencies, is_silent=True)\n",
    "        if history:\n",
    "            final_accuracy = history['accuracy'][-1]\n",
    "            depth_accuracies.append(final_accuracy)\n",
    "            print(f\"Final accuracy for depth {depth}: {final_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nAnalyzing effect of network width...\")\n",
    "    widths = [32, 64, 128, 256]\n",
    "    depth = 3\n",
    "    width_accuracies = []\n",
    "    for width in widths:\n",
    "        print(f\"Training model with depth {depth} and width {width}...\")\n",
    "        layers = [Linear(input_dim, width, ReLU)]\n",
    "        for _ in range(depth - 1):\n",
    "            layers.append(Linear(width, width, ReLU))\n",
    "        layers.append(Linear(width, 1, Sigmoid))\n",
    "        model = Model(layers)\n",
    "\n",
    "        history = training_procedure(model, dataset, epochs=150, batch_size=128, initial_learning_rate=0.01, patience=20, username=f\"Width_{width}\", num_frequencies=num_frequencies, is_silent=True)\n",
    "        if history:\n",
    "            final_accuracy = history['accuracy'][-1]\n",
    "            width_accuracies.append(final_accuracy)\n",
    "            print(f\"Final accuracy for width {width}: {final_accuracy:.4f}\")\n",
    "\n",
    "    plt.close('all')\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('Architecture Analysis')\n",
    "    ax1.plot(depths, depth_accuracies, marker='o')\n",
    "    ax1.set_title(\"Accuracy vs. Depth (Width=128)\")\n",
    "    ax1.set_xlabel(\"Number of Hidden Layers\"); ax1.set_ylabel(\"Final Accuracy\"); ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(widths, width_accuracies, marker='o')\n",
    "    ax2.set_title(\"Accuracy vs. Width (Depth=3)\")\n",
    "    ax2.set_xlabel(\"Neurons per Hidden Layer\"); ax2.set_ylabel(\"Final Accuracy\"); ax2.grid(True)\n",
    "    \n",
    "    analysis_plot_path = 'runs/architecture_analysis.png'\n",
    "    os.makedirs('runs', exist_ok=True)\n",
    "    plt.savefig(analysis_plot_path)\n",
    "    print(f\"Analysis plot saved to {analysis_plot_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# Final Challenge\n",
    "def execute_challenge(dataset):\n",
    "    \"\"\"Attempts to solve the final challenge goals.\"\"\"\n",
    "    print(\"\\nExecuting Final Challenge\")\n",
    "    num_frequencies = 10\n",
    "    input_dim = 4 * num_frequencies\n",
    "    \n",
    "    print(\"\\nGoal 1: Minimize model size for >91% accuracy.\")\n",
    "    small_model = Model([\n",
    "        Linear(input_dim, 64, ReLU),\n",
    "        Linear(64, 64, ReLU),\n",
    "        Linear(64, 1, Sigmoid)\n",
    "    ])\n",
    "    training_procedure(small_model, dataset, epochs=250, batch_size=128, initial_learning_rate=0.01, patience=30, username=\"SmallModel_Challenge\", num_frequencies=num_frequencies)\n",
    "\n",
    "    print(\"\\nGoal 2: Minimize training samples for >91% accuracy.\")\n",
    "    fast_model = Model([\n",
    "        Linear(input_dim, 256, ReLU),\n",
    "        Linear(256, 256, ReLU),\n",
    "        Linear(256, 1, Sigmoid)\n",
    "    ])\n",
    "    training_procedure(fast_model, dataset, epochs=100, batch_size=256, initial_learning_rate=0.02, patience=20, username=\"FastConverge_Challenge\", num_frequencies=num_frequencies)\n",
    "\n",
    "def main_additions():\n",
    "    \"\"\"Main function to execute the additional sections of Q1.\"\"\"\n",
    "    RUN_SANITY_CHECKS = True\n",
    "    RUN_ANALYSIS = False\n",
    "    RUN_CHALLENGE = False\n",
    "\n",
    "    if RUN_SANITY_CHECKS:\n",
    "        execute_xor_test()\n",
    "        execute_gradient_check()\n",
    "\n",
    "    # Create the dataset object only if needed for analysis or challenge\n",
    "    if RUN_ANALYSIS or RUN_CHALLENGE:\n",
    "        print(\"Creating border_dataset object for analysis/challenge...\")\n",
    "        border_dataset = BorderDataset(image_path=\"Dataset/border.png\")\n",
    "        if RUN_ANALYSIS:\n",
    "            execute_analysis(border_dataset)\n",
    "\n",
    "        if RUN_CHALLENGE:\n",
    "            execute_challenge(border_dataset)\n",
    "\n",
    "# This call will now run the functions in this cell.\n",
    "main_additions()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smai-env)",
   "language": "python",
   "name": "smai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
